{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Model U-Net Training for Methane Emission Segmentation\n",
    "### Annabel Simpson\n",
    "\n",
    "This notebook implements and trains three different U-Net models using RGB, RGB+Mag1c, and Multispectral inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Dependencies\n",
    "!pip install --quiet rasterio gdown fsspec omegaconf torch torchvision torchtext \\\n",
    "    pytorch-lightning segmentation_models_pytorch hydra-core geopandas \\\n",
    "    ipykernel matplotlib scikit-image scikit-learn wandb kornia==0.6.7 torchmetrics==0.10.0\n",
    "\n",
    "\n",
    "!pip install git+https://github.com/spaceml-org/georeader.git --quiet\n",
    "\n",
    "\n",
    "!git clone https://github.com/spaceml-org/STARCOP.git || echo \"Repository already exists\"\n",
    "\n",
    "%cd /content/\n",
    "\n",
    "#Download Data & Models\n",
    "!gdown https://drive.google.com/uc?id=1Qw96Drmk2jzBYSED0YPEUyuc2DnBechl -O STARCOP_mini.zip\n",
    "!gdown https://drive.google.com/uc?id=1TXFlAHO_eRdfbJGLNNt3KY0lJqjm3fdX -O multistarcop_varon.zip\n",
    "!gdown https://drive.google.com/uc?id=1Kvnc_lOBn4z-xO1HFRyLZOMEldXWQvql -O hyperstarcop_magic_rgb.zip\n",
    "\n",
    "#Unzip\n",
    "!unzip -qo STARCOP_mini.zip\n",
    "!unzip -qo multistarcop_varon.zip\n",
    "!unzip -qo hyperstarcop_magic_rgb.zip\n",
    "\n",
    "\n",
    "!rm -f *.zip\n",
    "\n",
    "\n",
    "!ls /content/hyperstarcop_magic_rgb\n",
    "!ls /content/multistarcop_varon\n",
    "\n",
    "\n",
    "%cd /content/STARCOP\n",
    "\n",
    "\n",
    "import omegaconf\n",
    "import pylab as plt\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import ast\n",
    "import pkgutil\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from starcop.data.datamodule import Permian2019DataModule\n",
    "from starcop.models.model_module import ModelModule\n",
    "\n",
    "#Set Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    import georeader\n",
    "    print(\"Spaceman Georeader found at:\", georeader.__file__)\n",
    "    modules = [module.name for module in pkgutil.iter_modules(georeader.__path__)]\n",
    "    print(\"Available Spaceman Georeader Modules:\", modules)\n",
    "\n",
    "    if 'slices' not in modules:\n",
    "        raise ModuleNotFoundError(\"Missing 'slices' module in Spaceman Georeader\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Spaceman Georeader module not found or missing 'slices'. Reinstalling...\")\n",
    "    !pip install --upgrade --force-reinstall git+https://github.com/spaceml-org/georeader.git --quiet\n",
    "    import georeader\n",
    "\n",
    "\n",
    "config_general = omegaconf.OmegaConf.load(\"scripts/configs/config.yaml\")\n",
    "root_folder = \"/content/STARCOP_mini\"\n",
    "\n",
    "# Func for Pre-trained Model with rgb+mag1c\n",
    "def load_model_with_datamodule(model_path, config_path):\n",
    "    config_model = omegaconf.OmegaConf.load(config_path)\n",
    "    config = omegaconf.OmegaConf.merge(config_general, config_model)\n",
    "\n",
    "    config_dict = omegaconf.OmegaConf.to_container(config_model, resolve=True)\n",
    "    print(\"Config Model Structure:\", config_dict) \n",
    "\n",
    "    dataset_str = config_dict.get(\"_content\", {}).get(\"value\", {}).get(\"dataset\", \"\")\n",
    "    try:\n",
    "        dataset_dict = ast.literal_eval(dataset_str)\n",
    "    except (SyntaxError, ValueError):\n",
    "        raise ValueError(\"Dataset configuration is not properly formatted as a dictionary\")\n",
    "\n",
    "    dataset_dict['root_folder'] = root_folder\n",
    "    dataset_dict['train_csv'] = '/content/STARCOP_mini/train_mini10.csv'\n",
    "    config.dataset = dataset_dict\n",
    "    config.products_plot = config_dict.get(\"products_plot\", {})\n",
    "\n",
    "    data_module = Permian2019DataModule(config)\n",
    "    data_module.test_csv = '/content/STARCOP_mini/test_mini10.csv'\n",
    "    data_module.settings['dataset'] = dataset_dict\n",
    "\n",
    "    data_module.prepare_data()\n",
    "\n",
    "    model = ModelModule.load_from_checkpoint(model_path, settings=config)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"Loaded {os.path.basename(model_path)} with {model.num_channels} input channels\")\n",
    "    return model, data_module, config\n",
    "\n",
    "# HyperSTARCOP Model with rgb+mag1c\n",
    "hsi_model_path = \"/content/hyperstarcop_magic_rgb/final_checkpoint_model.ckpt\"\n",
    "hsi_config_path = \"/content/hyperstarcop_magic_rgb/config.yaml\"\n",
    "hsi_model, hsi_dm, hsi_config = load_model_with_datamodule(hsi_model_path, hsi_config_path)\n",
    "print(\"Successfully loaded HyperSTARCOP model!\")\n",
    "\n",
    "# MultiSTARCOP Model (TOA_WV3_* or TOA_AVIRIS_*)\n",
    "msi_model_path = \"/content/multistarcop_varon/final_checkpoint_model.ckpt\"\n",
    "msi_config_path = \"/content/multistarcop_varon/config.yaml\"\n",
    "msi_model, msi_dm, msi_config = load_model_with_datamodule(msi_model_path, msi_config_path)\n",
    "print(\"Successfully loaded MultiSTARCOP model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def load_raster(image_path):\n",
    "    \"\"\"Loads a single band from a raster file.\"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        return None\n",
    "    with rasterio.open(image_path) as src:\n",
    "        return src.read(1).astype(np.float32)  \n",
    "\n",
    "\n",
    "def normalize(img):\n",
    "    \"\"\"Normalize image data to [0,1] for display.\"\"\"\n",
    "    if img is None or img.max() == 0:\n",
    "        return None  \n",
    "    return (img - img.min()) / (img.max() - img.min() + 1e-8) \n",
    "\n",
    "\n",
    "def get_image_paths(folder_path):\n",
    "    \"\"\"Gets the paths to available spectral bands inside a given folder, including Mag1c.\"\"\"\n",
    "    return {\n",
    "        \"RGB Composite\": [os.path.join(folder_path, f\"TOA_AVIRIS_{wavelength}nm.tif\") for wavelength in [\"640\", \"550\", \"460\"]],\n",
    "        \"Ground Truth\": os.path.join(folder_path, \"labelbinary.tif\"),\n",
    "        \"Mag1c\": os.path.join(folder_path, \"mag1c.tif\"), \n",
    "        \"AVIRIS_2004\": os.path.join(folder_path, \"TOA_AVIRIS_2004nm.tif\"),\n",
    "        \"AVIRIS_2109\": os.path.join(folder_path, \"TOA_AVIRIS_2109nm.tif\"),\n",
    "        \"AVIRIS_2310\": os.path.join(folder_path, \"TOA_AVIRIS_2310nm.tif\"),\n",
    "        \"AVIRIS_2350\": os.path.join(folder_path, \"TOA_AVIRIS_2350nm.tif\"),\n",
    "        \"AVIRIS_2360\": os.path.join(folder_path, \"TOA_AVIRIS_2360nm.tif\"),\n",
    "        \"SWIR1\": os.path.join(folder_path, \"TOA_WV3_SWIR1.tif\"),\n",
    "        \"SWIR2\": os.path.join(folder_path, \"TOA_WV3_SWIR2.tif\"),\n",
    "        \"SWIR3\": os.path.join(folder_path, \"TOA_WV3_SWIR3.tif\"),\n",
    "        \"SWIR4\": os.path.join(folder_path, \"TOA_WV3_SWIR4.tif\"),\n",
    "        \"SWIR5\": os.path.join(folder_path, \"TOA_WV3_SWIR5.tif\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def visualize_more_bands(folder_path):\n",
    "    \"\"\"Displays multiple bands including AVIRIS, SWIR, and Mag1c bands.\"\"\"\n",
    "    paths = get_image_paths(folder_path)\n",
    "    loaded_bands = {}\n",
    "\n",
    "\n",
    "    rgb_bands = [load_raster(path) for path in paths[\"RGB Composite\"]]\n",
    "    if all(b is not None for b in rgb_bands):\n",
    "        rgb_image = np.stack(rgb_bands, axis=-1)\n",
    "        rgb_image = normalize(rgb_image)  \n",
    "        loaded_bands[\"RGB Composite\"] = rgb_image\n",
    "\n",
    "    # Load other bands\n",
    "    for key, path in paths.items():\n",
    "        if key == \"RGB Composite\":  \n",
    "            continue  \n",
    "\n",
    "        band = load_raster(path)\n",
    "        if band is not None:\n",
    "            loaded_bands[key] = normalize(band)\n",
    "\n",
    "    # Remove any bands that failed to load\n",
    "    loaded_bands = {k: v for k, v in loaded_bands.items() if v is not None}\n",
    "\n",
    "    # Ensure at least one band is available\n",
    "    if not loaded_bands:\n",
    "        print(\"No valid bands found for visualization.\")\n",
    "        return\n",
    "\n",
    "    # Create figure dynamically based on number of available bands\n",
    "    num_bands = len(loaded_bands)\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=num_bands, figsize=(20, 5))\n",
    "\n",
    "    # Handle single band case\n",
    "    if num_bands == 1:\n",
    "        axes = [axes]  \n",
    "\n",
    "    for ax, (band_name, band_data) in zip(axes, loaded_bands.items()):\n",
    "        ax.imshow(band_data, cmap=\"gray\" if \"Ground Truth\" in band_name else \"viridis\")\n",
    "        ax.set_title(band_name)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "sample_folder = \"/content/STARCOP_mini/ang20191018t165503_r2660_c460_w151_h151\"\n",
    "visualize_more_bands(sample_folder)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rasterio matplotlib pandas numpy folium\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import folium\n",
    "from rasterio.warp import transform_bounds\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# Define dataset paths\n",
    "dataset_folder = \"/content/drive/MyDrive\"\n",
    "zip_path = \"/content/drive/MyDrive/STARCOPtrain.zip\"\n",
    "extract_path = \"/content/starcop_data\"\n",
    "\n",
    "# Extract\n",
    "if os.path.exists(zip_path) and not os.path.exists(extract_path):\n",
    "    print(\"Dataset found. Extracting...\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(\"Extraction complete!\")\n",
    "elif os.path.exists(extract_path):\n",
    "    print(\"Dataset already extracted!\")\n",
    "else:\n",
    "    print(\"Dataset not found! Check the path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "train_csv_path = \"/content/drive/MyDrive/AL4CC/AL4CC/train.csv\"\n",
    "test_csv_path = \"/content/drive/MyDrive/AL4CC/AL4CC/test.csv\"\n",
    "train_data_dir = \"/content/starcop_data/STARCOP_train_easy\"\n",
    "test_data_dir = \"/content/drive/MyDrive/STARCOP_test\"\n",
    "\n",
    "# Load CSVs\n",
    "df_train = pd.read_csv(train_csv_path)\n",
    "df_test = pd.read_csv(test_csv_path)\n",
    "\n",
    "# Get actual available folders\n",
    "available_train_folders = set(os.listdir(train_data_dir))\n",
    "available_test_folders = set(os.listdir(test_data_dir))\n",
    "\n",
    "# Ensure folder name column exists\n",
    "df_train[\"folder_name\"] = df_train[\"folder\"].apply(lambda x: os.path.basename(str(x)))\n",
    "df_test[\"folder_name\"] = df_test[\"folder\"].apply(lambda x: os.path.basename(str(x)))\n",
    "\n",
    "# Filter to only include available folders\n",
    "df_train_filtered = df_train[df_train[\"folder_name\"].isin(available_train_folders)].copy().reset_index(drop=True)\n",
    "df_test_filtered = df_test[df_test[\"folder_name\"].isin(available_test_folders)].copy().reset_index(drop=True)\n",
    "\n",
    "# Save updated CSVs\n",
    "filtered_train_path = \"/content/drive/MyDrive/AL4CC/AL4CC/train_filtered.csv\"\n",
    "filtered_test_path = \"/content/drive/MyDrive/AL4CC/AL4CC/test_filtered.csv\"\n",
    "\n",
    "df_train_filtered.to_csv(filtered_train_path, index=False)\n",
    "df_test_filtered.to_csv(filtered_test_path, index=False)\n",
    "\n",
    "print(f\"Updated train CSV: {df_train_filtered.shape}\")\n",
    "print(f\"Updated test CSV: {df_test_filtered.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "\n",
    "def load_raster(image_path):\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Missing file: {image_path}\")\n",
    "        return None\n",
    "    with rasterio.open(image_path) as src:\n",
    "        return src.read(1)\n",
    "\n",
    "\n",
    "def normalize_image(img):\n",
    "    \"\"\" Normalize image to 0-255 and convert to uint8 for display. \"\"\"\n",
    "    if img is None or np.max(img) == 0:\n",
    "        return None  \n",
    "    img = (img - np.min(img)) / (np.max(img) - np.min(img) + 1e-8)  \n",
    "    return (img * 255).astype(np.uint8)  \n",
    "\n",
    "def enhance_contrast(img):\n",
    "    \"\"\" Apply histogram equalization for better contrast. \"\"\"\n",
    "    if img is None:\n",
    "        return None\n",
    "    img = normalize_image(img)\n",
    "    return cv2.equalizeHist(img)\n",
    "\n",
    "\n",
    "def get_image_paths(event_id, train_data_dir):\n",
    "    folder_path = os.path.join(train_data_dir, event_id)\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(f\"Skipping {event_id} (Folder missing)\")\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"R\": os.path.join(folder_path, \"TOA_AVIRIS_640nm.tif\"),\n",
    "        \"G\": os.path.join(folder_path, \"TOA_AVIRIS_550nm.tif\"),\n",
    "        \"B\": os.path.join(folder_path, \"TOA_AVIRIS_460nm.tif\"),\n",
    "        \"Mag1c\": os.path.join(folder_path, \"mag1c.tif\"),\n",
    "        \"GT\": os.path.join(folder_path, \"labelbinary.tif\"),\n",
    "    }\n",
    "\n",
    "\n",
    "def visualize_more_bands(event_id, train_data_dir):\n",
    "    paths = get_image_paths(event_id, train_data_dir)\n",
    "    if paths is None:\n",
    "        return\n",
    "    \n",
    "    # Load images\n",
    "    loaded_bands = {key: load_raster(paths[key]) for key in paths if os.path.exists(paths[key])}\n",
    "    \n",
    "    # Ensure all images exist\n",
    "    required_bands = [\"R\", \"G\", \"B\", \"Mag1c\", \"GT\"]\n",
    "    if any(b not in loaded_bands for b in required_bands):\n",
    "        print(f\"Skipping {event_id} (missing essential bands)\")\n",
    "        return\n",
    "\n",
    "    # Normalize RGB and Mag1c\n",
    "    r, g, b = map(normalize_image, [loaded_bands[\"R\"], loaded_bands[\"G\"], loaded_bands[\"B\"]])\n",
    "    mag1c = enhance_contrast(loaded_bands[\"Mag1c\"])\n",
    "\n",
    "    # Normalize GT Mask (Ensure binary 0/1) - Convert mask to binary\n",
    "    gt = loaded_bands[\"GT\"]\n",
    "    gt = (gt > 0).astype(np.uint8) \n",
    "\n",
    "    # Stack RGB + Mag1c\n",
    "    rgb_mag1c_image = np.stack([r, g, b], axis=-1)\n",
    "\n",
    "    # Plot Images\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # RGB Composite\n",
    "    axes[0].imshow(rgb_mag1c_image)\n",
    "    axes[0].set_title(\"RGB Composite\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    # Mag1c Band\n",
    "    im = axes[1].imshow(mag1c, cmap=\"magma\")\n",
    "    axes[1].set_title(\"Enhanced Mag1c\")\n",
    "    axes[1].axis(\"off\")\n",
    "    plt.colorbar(im, ax=axes[1])\n",
    "\n",
    "    # GT Mask\n",
    "    im = axes[2].imshow(gt, cmap=\"gray\")\n",
    "    axes[2].set_title(\"Ground Truth (Binary Mask)\")\n",
    "    axes[2].axis(\"off\")\n",
    "    plt.colorbar(im, ax=axes[2])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show(block=True)  \n",
    "    print(\"Display successful!\")\n",
    "\n",
    "# Test\n",
    "visualize_more_bands(\"ang20191010t192008_r8444_c339_w151_h151\", \"/content/starcop_data/STARCOP_train_easy\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folium map centered at the Permian Basin\n",
    "map_plumes = folium.Map(location=[31.7, -103.6], zoom_start=8)\n",
    "\n",
    "for idx, row in df_train_filtered.iterrows():\n",
    "    folder_path = os.path.join(train_data_dir, row[\"folder_name\"])\n",
    "    label_path = os.path.join(folder_path, \"labelbinary.tif\")\n",
    "\n",
    "    # Skip missing labels\n",
    "    if not os.path.exists(label_path):\n",
    "        print(f\"Skipping {row['folder_name']} (no labelbinary.tif)\")\n",
    "        continue\n",
    "\n",
    "    with rasterio.open(label_path) as src:\n",
    "        bounds_utm = src.bounds\n",
    "        bounds_lng_lat = transform_bounds(src.crs, \"EPSG:4326\", *bounds_utm)\n",
    "        lat, lon = (bounds_lng_lat[1] + bounds_lng_lat[3]) / 2, (bounds_lng_lat[0] + bounds_lng_lat[2]) / 2\n",
    "\n",
    "    folium.Circle(\n",
    "        location=[lat, lon],\n",
    "        radius=500,\n",
    "        color='red',\n",
    "        fill=True,\n",
    "        popup=f\"Event {row['folder_name']}\"\n",
    "    ).add_to(map_plumes)\n",
    "\n",
    "map_plumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Function to load raster images\n",
    "def load_raster(image_path):\n",
    "    if not os.path.exists(image_path):\n",
    "        return None\n",
    "    with rasterio.open(image_path) as src:\n",
    "        return src.read(1)\n",
    "\n",
    "# Methane Emission Dataset with Multi-Spectral Inputs\n",
    "class MethaneEmissionDataset(Dataset):\n",
    "    def __init__(self, df, train_data_dir, mode=\"rgb+mag1c\", transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.train_data_dir = train_data_dir\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        folder_path = os.path.join(self.train_data_dir, row[\"folder_name\"])\n",
    "\n",
    "        # Load bands\n",
    "        def load(name):\n",
    "            return load_raster(os.path.join(folder_path, name))\n",
    "\n",
    "        r = load(\"TOA_AVIRIS_640nm.tif\")\n",
    "        g = load(\"TOA_AVIRIS_550nm.tif\")\n",
    "        b = load(\"TOA_AVIRIS_460nm.tif\")\n",
    "        mag1c = load(\"mag1c.tif\")\n",
    "        gt = load(\"labelbinary.tif\")\n",
    "\n",
    "        if any(x is None for x in [r, g, b, gt]):\n",
    "            return self.__getitem__((idx + 1) % len(self.df))\n",
    "\n",
    "        def normalize(img):\n",
    "            return img / np.max(img) if np.max(img) > 0 else img\n",
    "\n",
    "        r, g, b = map(normalize, [r, g, b])\n",
    "        rgb = torch.tensor(np.stack([r, g, b], axis=-1), dtype=torch.float32).permute(2, 0, 1)\n",
    "\n",
    "        if self.transform:\n",
    "            rgb = self.transform(rgb)\n",
    "\n",
    "        if self.mode == \"rgb\":\n",
    "            final_image = rgb\n",
    "        elif self.mode == \"rgb+mag1c\":\n",
    "            mag1c = normalize(mag1c)\n",
    "            mag1c_tensor = torch.tensor(mag1c, dtype=torch.float32).unsqueeze(0)\n",
    "            final_image = torch.cat([rgb, mag1c_tensor], dim=0)\n",
    "        elif self.mode == \"multispectral\":\n",
    "            av_2004 = load(\"TOA_AVIRIS_2004nm.tif\")\n",
    "            swir1 = load(\"TOA_WV3_SWIR1.tif\")\n",
    "            if any(x is None for x in [mag1c, av_2004, swir1]):\n",
    "                return self.__getitem__((idx + 1) % len(self.df))\n",
    "            mag1c = normalize(mag1c)\n",
    "            av_2004 = normalize(av_2004)\n",
    "            swir1 = normalize(swir1)\n",
    "            additional = torch.tensor(np.stack([mag1c, av_2004, swir1]), dtype=torch.float32)\n",
    "            final_image = torch.cat([rgb, additional], dim=0)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode: {self.mode}\")\n",
    "\n",
    "        gt_tensor = torch.tensor(gt, dtype=torch.float32).unsqueeze(0)\n",
    "        gt_tensor[gt_tensor > 0] = 1\n",
    "        return final_image, gt_tensor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Data transformations (Apply only on RGB)\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees=30),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "])\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.8 * len(df_train_filtered))\n",
    "val_size = len(df_train_filtered) - train_size\n",
    "\n",
    "train_dataset = MethaneEmissionDataset(df_train_filtered.iloc[:train_size], train_data_dir, transform=data_transforms)\n",
    "val_dataset = MethaneEmissionDataset(df_train_filtered.iloc[train_size:], train_data_dir, transform=None)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import os\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "\n",
    "def visualize_multispectral(folder_path):\n",
    "    # Load images\n",
    "    r = load_raster(os.path.join(folder_path, \"TOA_AVIRIS_640nm.tif\"))\n",
    "    g = load_raster(os.path.join(folder_path, \"TOA_AVIRIS_550nm.tif\"))\n",
    "    b = load_raster(os.path.join(folder_path, \"TOA_AVIRIS_460nm.tif\"))\n",
    "    mag1c = load_raster(os.path.join(folder_path, \"mag1c.tif\"))\n",
    "    aviris_2004 = load_raster(os.path.join(folder_path, \"TOA_AVIRIS_2004nm.tif\"))\n",
    "    swir1 = load_raster(os.path.join(folder_path, \"TOA_WV3_SWIR1.tif\"))\n",
    "    gt = load_raster(os.path.join(folder_path, \"labelbinary.tif\"))\n",
    "\n",
    "\n",
    "    if any(x is None for x in [r, g, b, mag1c, aviris_2004, swir1, gt]):\n",
    "        print(\"Skipping due to missing data.\")\n",
    "        return\n",
    "\n",
    "    # Normalize images\n",
    "    def normalize(img):\n",
    "        return img / np.max(img) if np.max(img) > 0 else img\n",
    "\n",
    "    r, g, b, mag1c, aviris_2004, swir1 = map(normalize, [r, g, b, mag1c, aviris_2004, swir1])\n",
    "\n",
    "    # Stack RGB\n",
    "    rgb = np.stack([r, g, b], axis=-1)\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "    # RGB Composite\n",
    "    axes[0, 0].imshow(rgb)\n",
    "    axes[0, 0].set_title(\"RGB Composite\")\n",
    "    axes[0, 0].axis(\"off\")\n",
    "\n",
    "    # Mag1c\n",
    "    axes[0, 1].imshow(mag1c, cmap=\"magma\")\n",
    "    axes[0, 1].set_title(\"Mag1c\")\n",
    "    axes[0, 1].axis(\"off\")\n",
    "\n",
    "    # AVIRIS 2004nm [hyperspectral]\n",
    "    axes[0, 2].imshow(aviris_2004, cmap=\"viridis\")\n",
    "    axes[0, 2].set_title(\"AVIRIS 2004nm\")\n",
    "    axes[0, 2].axis(\"off\")\n",
    "\n",
    "    # SWIR1 [multispectral]\n",
    "    axes[1, 0].imshow(swir1, cmap=\"inferno\")\n",
    "    axes[1, 0].set_title(\"SWIR1\")\n",
    "    axes[1, 0].axis(\"off\")\n",
    "\n",
    "    # GT Mask\n",
    "    im = axes[1, 1].imshow(gt, cmap=\"gray\")\n",
    "    axes[1, 1].set_title(\"Ground Truth Mask\")\n",
    "    axes[1, 1].axis(\"off\")\n",
    "    divider = make_axes_locatable(axes[1, 1])\n",
    "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "    plt.colorbar(im, cax=cax)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "  \n",
    "  # Example usage with a valid folder path\n",
    "sample_folder = \"/content/starcop_data/STARCOP_train_easy/ang20190923t174142_r4096_c0_w512_h512\"\n",
    "visualize_multispectral(sample_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imgs, masks in train_loader:\n",
    "    print(f\"Input batch shape: {imgs.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MethaneEmissionDataset(Dataset):\n",
    "    def __init__(self, df, train_data_dir, mode=\"rgb\", transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.train_data_dir = train_data_dir\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        folder_path = os.path.join(self.train_data_dir, row[\"folder_name\"])\n",
    "\n",
    "        def load_image(name):\n",
    "            path = os.path.join(folder_path, name)\n",
    "            if os.path.exists(path):\n",
    "                with rasterio.open(path) as src:\n",
    "                    return src.read(1).astype(np.float32)\n",
    "            return None\n",
    "\n",
    "        # Load channels\n",
    "        r = load_image(\"TOA_AVIRIS_640nm.tif\")\n",
    "        g = load_image(\"TOA_AVIRIS_550nm.tif\")\n",
    "        b = load_image(\"TOA_AVIRIS_460nm.tif\")\n",
    "        mag1c = load_image(\"mag1c.tif\")\n",
    "        av_2004 = load_image(\"TOA_AVIRIS_2004nm.tif\")\n",
    "        swir1 = load_image(\"TOA_WV3_SWIR1.tif\")\n",
    "        gt = load_image(\"labelbinary.tif\")\n",
    "\n",
    "        if any(x is None for x in [r, g, b, gt]):\n",
    "            return self.__getitem__((idx + 1) % len(self.df))\n",
    "\n",
    "        # Normalize images\n",
    "        def normalize(img):\n",
    "            return img / np.max(img) if np.max(img) > 0 else img\n",
    "\n",
    "        r, g, b = map(normalize, [r, g, b])\n",
    "        gt = (gt > 0).astype(np.float32)\n",
    "\n",
    "        # Select input mode\n",
    "        if self.mode == \"rgb\":\n",
    "            input_tensor = torch.tensor(np.stack([r, g, b], axis=-1), dtype=torch.float32).permute(2, 0, 1)\n",
    "        elif self.mode == \"rgb+mag1c\":\n",
    "            mag1c = normalize(mag1c)\n",
    "            input_tensor = torch.tensor(np.stack([r, g, b, mag1c], axis=-1), dtype=torch.float32).permute(2, 0, 1)\n",
    "        elif self.mode == \"multispectral\":\n",
    "            if any(x is None for x in [mag1c, av_2004, swir1]):\n",
    "                return self.__getitem__((idx + 1) % len(self.df))\n",
    "            mag1c, av_2004, swir1 = map(normalize, [mag1c, av_2004, swir1])\n",
    "            input_tensor = torch.tensor(np.stack([r, g, b, mag1c, av_2004, swir1], axis=-1), dtype=torch.float32).permute(2, 0, 1)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid mode: {self.mode}\")\n",
    "\n",
    "        gt_tensor = torch.tensor(gt, dtype=torch.float32).unsqueeze(0)\n",
    "        return input_tensor, gt_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_classes=1, in_channels=3):\n",
    "        super(UNet, self).__init__()\n",
    "        self.encoder = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        \n",
    "        # Modify input layer to handle different in_channels\n",
    "        self.encoder.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        # Encoder layers (ResNet outputs)\n",
    "        self.enc1 = self.encoder.layer1  \n",
    "        self.enc2 = self.encoder.layer2  \n",
    "        self.enc3 = self.encoder.layer3  \n",
    "        self.enc4 = self.encoder.layer4  \n",
    "\n",
    "        # Decoder w/proper upsampling\n",
    "        self.upconv1 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.conv1 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.encoder.relu(self.encoder.bn1(self.encoder.conv1(x)))  \n",
    "        x2 = self.enc1(x1)  \n",
    "        x3 = self.enc2(x2)  \n",
    "        x4 = self.enc3(x3)  \n",
    "        x5 = self.enc4(x4)  \n",
    "\n",
    "\n",
    "        x = self.upconv1(x5)  \n",
    "        x = torch.cat([x, x4], dim=1)  \n",
    "        x = self.bn1(F.relu(self.conv1(x)))\n",
    "\n",
    "        x = self.upconv2(x)  \n",
    "        x = torch.cat([x, x3], dim=1)\n",
    "        x = self.bn2(F.relu(self.conv2(x)))\n",
    "\n",
    "        x = self.upconv3(x)  \n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        x = self.bn3(F.relu(self.conv3(x)))\n",
    "\n",
    "        x = self.final_conv(x)  \n",
    "        return torch.sigmoid(x)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=5, lr=0.001):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Define loss function inside\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} - Training\")\n",
    "        for images, masks in loop:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Resize masks to match output size\n",
    "            masks_resized = F.interpolate(masks, size=outputs.shape[2:], mode=\"nearest\")\n",
    "\n",
    "            loss = criterion(outputs, masks_resized)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            loop.set_postfix(loss=train_loss / len(train_loader))\n",
    "\n",
    "        # Validation \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                masks_resized = F.interpolate(masks, size=outputs.shape[2:], mode=\"nearest\")\n",
    "                loss = criterion(outputs, masks_resized)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] | Train Loss: {train_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGB Model (in_channels=3)\n",
    "train_dataset_rgb = MethaneEmissionDataset(df_train_filtered, train_data_dir, mode=\"rgb\")  \n",
    "train_loader_rgb = torch.utils.data.DataLoader(train_dataset_rgb, batch_size=8, shuffle=True, num_workers=2)\n",
    "model_rgb = UNet(in_channels=3)\n",
    "model_rgb = train_model(model_rgb, train_loader_rgb, val_loader, num_epochs=5)\n",
    "\n",
    "# RGB + Mag1c Model (in_channels=4)\n",
    "train_dataset_rgb_mag1c = MethaneEmissionDataset(df_train_filtered, train_data_dir, mode=\"rgb+mag1c\")  \n",
    "train_loader_rgb_mag1c = torch.utils.data.DataLoader(train_dataset_rgb_mag1c, batch_size=8, shuffle=True, num_workers=2)\n",
    "model_rgb_mag1c = UNet(in_channels=4)\n",
    "model_rgb_mag1c = train_model(model_rgb_mag1c, train_loader_rgb_mag1c, val_loader, num_epochs=5)\n",
    "\n",
    "# Multispectral Model (in_channels=6)\n",
    "train_dataset_multispectral = MethaneEmissionDataset(df_train_filtered, train_data_dir, mode=\"multispectral\")  \n",
    "train_loader_multispectral = torch.utils.data.DataLoader(train_dataset_multispectral, batch_size=8, shuffle=True, num_workers=2)\n",
    "model_multispectral = UNet(in_channels=6)\n",
    "model_multispectral = train_model(model_multispectral, train_loader_multispectral, val_loader, num_epochs=5)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
